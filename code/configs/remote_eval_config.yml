continue_training: False # this is for resuming training
# load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20241206_001843_i2wyhh_kmh-tpuvm-v3-32-1__b_lr_ep_eval # FM model (w/ t)
# load_from: /kmh-nfs-us-mount/logs/sqa/sqa_Flow_matching/20241213_190021_ftvrxo_kmh-tpuvm-v2-32-2__b_lr_ep_eval # New FM model (w/ t)
# load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20241207_160106_l7h71r_kmh-tpuvm-v3-32-1__b_lr_ep_eval # DDPM model (w/ t)
load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20241210_003420_srefmk_kmh-tpuvm-v3-32-preemptible-1__b_lr_ep_eval/best_fid # ADM model (w/ t)
# load_from: /kmh-nfs-us-mount/logs/sqa/sqa_Flow_matching/20241210_003719_b663n5_kmh-tpuvm-v2-32-7__b_lr_ep_eval/best_fid # ADM class-conditional model (w/ t)
############################################
#     Normal Training Configuration        #
############################################
model:
    ##### T-conditioning #####
    t_condition_method: direct # options: [direct, log999, not (i.e. no t)]
    
    ##### Common Configs (rarely changes) #####
    image_size: 32
    out_channels: 3
    base_width: 128
    use_aug_label: False
    net_type: ncsnppedm
    average_loss: True # use average or sum for loss

    ##### Common Configs (may changes) #####
    dropout: 0.15
    embedding_type: positional
    task: Diffusion # options: [FM, Diffusion]

    ##### FM-specific configs #####

    ##### Diffusion-specific configs #####
    learn_var: True # default: False

    ##### Conditional Generation #####
    class_conditional: False

    ##### Sampling Configs #####
    n_T: 250 # num of sample steps, matches "diffusion_nT"
    
    # --- Flow Matching --- #
    # sampler: euler
    # sampler: heun
    # sampler: edm
    # sampler: edm-sde
    # ode_solver: jax
    # ode_solver: O

    # --- Diffusion --- #
    sampler: ddpm
    # sampler: DDIM
    sample_clip_denoised: True

diffusion:
    diffusion_nT: 4000
    diffusion_schedule: cosine

dataset:
    name: cifar10
    root: CIFAR # pytorch
    prefetch_factor: 2
    num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly
    cache: True

aug: 
    use_edm_aug: False # NOTE: if you want to turn on this, also turn on model.use_aug_label

fid:
    eval_only: False
    fid_per_epoch: 200
    num_samples: 50000
    cache_ref: /kmh-nfs-us-mount/data/cached/cifar10_jax_stats_20240820.npz # pytorch

evalu: # In principle, this should not be used
    ema: True
    sample: True

batch_size: 2048
num_epochs: 2000
learning_rate: 0.0008
lr_schedule: const
weight_decay: 0.0
optimizer: adamw
adam_b2: 0.95
grad_clip: 0.0
warmup_epochs: 200
log_per_step: 20
visualize_per_epoch: 50
eval_per_epoch: 100
checkpoint_per_epoch: 500
save_by_fid: False # if True, then only save checkpoint with fid better than all previous checkpoints

##### Classifier Guidance #####
run_classifier_guidance: True
# load_classifier_from: /kmh-nfs-us-mount/logs/sqa/sqa_Flow_matching/20241206_233913_v8zrw5_kmh-tpuvm-v2-32-preemptible-1__b_lr_ep_eval # this classifier has lowest valid loss
load_classifier_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20241215_025006_1tmfkz_kmh-tpuvm-v2-32-4__b_lr_ep_eval # a new classifier for testing
classifier_scale: 5.0
classifier_model:
    task: Classifier
    image_size: 32
    out_channels: 3
    base_width: 32
    n_T: 250 # num of sample steps, matches "diffusion_nT"
    dropout: 0.3
    net_type: ncsnppedm_classifier
    average_loss: True # use average or sum for loss

    t_condition_method: direct # default: log999
    classifier_model_depth: 3
