# Here is for CIFAR10:
model:
    image_size: 32
    out_channels: 3
    base_width: 128
    n_T: 1000
    use_aug_label: False
    dropout: 0.20
    net_type: ncsnppedm
    average_loss: True # use average or sum for loss
    # sampler: euler
    # sampler: heun
    # sampler: edm
    # sampler: edm-sde
    # sampler: DDIM
    sampler: DDIM_t
    ode_solver: jax
    # ode_solver: scipy # rk45
    no_condition_t: False
dataset:
    name: cifar10
    root: CIFAR # pytorch
    prefetch_factor: 2
    num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly
    cache: False
aug: 
    use_edm_aug: False # NOTE: if you want to turn on this, also turn on model.use_aug_label
fid:
    eval_only: False
    fid_per_epoch: 100
    num_samples: 1000
    cache_ref: /kmh-nfs-us-mount/data/cached/cifar10_jax_stats_20240820.npz # pytorch
evalu:
    ema: True
    sample: True
batch_size: 512
num_epochs: 400
learning_rate: 0.0008
lr_schedule: const
weight_decay: 0
optimizer: adamw
adam_b2: 0.999
grad_clip: 1.0
warmup_epochs: 200
log_per_step: 30
visualize_per_epoch: 100
eval_per_epoch: 100
wandb: False
checkpoint_per_epoch: 100
load_from: /kmh-nfs-us-mount/logs/sqa/sqa_Flow_matching/20241209_004559_gihhxu_kmh-tpuvm-v2-32-1__b_lr_ep_eval/checkpoint_194000

