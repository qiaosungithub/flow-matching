# Put your `run_remote.sh` configs in this YAML file

# # Here is for MNIST:
# training:
#     learning_rate: 0.0005
#     scheduler: poly # constant
#     warmup_steps: 20000
#     num_epochs: 500
#     batch_size: 1024
#     log_per_step: 20
#     sigma_min: 0.0
#     wandb: True
#     checkpoint_per_epoch: 1000
#     eval_per_epoch: 5
#     # load_from: /kmh-nfs-us-mount/logs/sqa/sqa_NCSNv2/20241101_030317_2ml2nf_kmh-tpuvm-v2-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_5684
# model:
#     name: UNet_for_mnist
# dataset:
#     name: MNIST
#     root: /kmh-nfs-ssd-eu-mount/code/qiao/data/MNIST/
#     image_size: 28
#     prefetch_factor: 2
#     num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly
# sampling:
#     ema: True
#     ema_decay: 0.999
#     save_dir: null
# fid:
#     eval_only: True
#     fid_per_epoch: 10

# Here is for CIFAR10:
model:
    image_size: 32
    out_channels: 3
    base_width: 128
    n_T: 100
    use_aug_label: False
    dropout: 0.3 # CT
    net_type: ncsnppedm
    average_loss: True # use average or sum for loss

    # sampler: euler
    # sampler: heun
    # sampler: edm
    # sampler: edm-sde
    sampler: DDIM
    ode_solver: jax
    # ode_solver: scipy # rk45
    
    no_condition_t: False
    # CT
    fourier_scale: 0.02 # CT
    embedding_type: positional # TODO: try fourier
    t_sampling: icm # or original
ct:
    start_scales: 10
    end_scales: 1280
    n_schedule: exp
dataset:
    name: cifar10
    # root: CIFAR # pytorch
    prefetch_factor: 2
    num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly
    cache: True
aug: 
    use_edm_aug: False
fid:
    eval_only: False
    fid_per_epoch: 50
    num_samples: 50000
    # cache_ref: /kmh-nfs-us-mount/data/cached/cifar10_jax_stats_20240820.npz # pytorch
evalu:
    ema: True
    sample: False
batch_size: 512
num_epochs: 3000
learning_rate: 0.0004 # CT
lr_schedule: const
weight_decay: 0.0
optimizer: radam # CT, TODO: radam / adamw?
adam_b2: 0.999
grad_clip: 0.0
warmup_epochs: 0 # CT
log_per_step: 100
visualize_per_epoch: 50
eval_per_epoch: 50
checkpoint_per_epoch: 500
# load_from: /kmh-nfs-ssd-eu-mount/logs/sqa/sqa_Flow_matching/20241202_214940_zd8s4j_kmh-tpuvm-v3-32-1__b_lr_ep_eval/checkpoint_48500

# # Here is for IMAGENET32x32:

# training:
#     learning_rate: 0.0001
#     scheduler: poly # constant
#     warmup_steps: 20000
#     num_epochs: 200
#     batch_size: 1024
#     log_per_step: 20
#     sigma_min: 0.0
#     wandb: True
#     checkpoint_per_epoch: 1000
#     eval_per_epoch: 5
#     # load_from: /kmh-nfs-us-mount/logs/sqa/sqa_NCSNv2/20241101_030317_2ml2nf_kmh-tpuvm-v2-32-1__b_lr_ep_torchvision_r50_eval/checkpoint_5684
# model:
#     name: UNet_for_32
# dataset:
#     name: imagenet2012:5.*.*
#     # root: /kmh-nfs-ssd-eu-mount/code/qiao/data/MNIST/
#     image_size: 32
#     prefetch_factor: 2
#     num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly
# sampling:
#     ema: True
#     ema_decay: 0.999
#     save_dir: null
# fid:
#     eval_only: True
#     fid_per_epoch: 10

# NOTE: you cannot add more hierarchy structure without modifying default.py and load_config.py