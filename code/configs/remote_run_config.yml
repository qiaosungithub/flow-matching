# Here is for CIFAR10:
model:
    image_size: 32
    out_channels: 3
    base_width: 128
    n_T: 1
    use_aug_label: False
    dropout: 0.3 # ECM
    net_type: ncsnppedm
    average_loss: False

    no_condition_t: False
    # CT
    fourier_scale: 0.02 # CT

    # embedding_type: positional2
    embedding_type: fourier

    t_sampling: ecm
    # t_sampling: icm

    # loss_type: "l2" 
    loss_type: "huber"
ct:
    start_scales: 10
    end_scales: 1280
    n_schedule: exp
dataset:
    name: cifar10
    root: CIFAR # pytorch
    prefetch_factor: 2
    num_workers: 64 # NOTE: On V2, if you don't use num_workers=64, sometimes the code will exit unexpectedly
    cache: True
aug: 
    use_edm_aug: False
fid:
    eval_only: False
    fid_per_epoch: 100
    num_samples: 50000
    cache_ref: /kmh-nfs-us-mount/data/cached/cifar10_jax_stats_20240820.npz # pytorch
evalu:
    ema: True
    sample: False
ema_value: 0.99993 # ICM
# ema_value: 0.9993 # ECM
batch_size: 1024
num_epochs: 4000
# learning_rate: 0.0004
learning_rate: 0.0001
lr_schedule: const
weight_decay: 0.0
optimizer: radam # CT & ECM
# optimizer: adamw
nan_to_num: True # ECM
adam_b2: 0.95
grad_clip: 0.0
warmup_epochs: 0 # CT & ECM
log_per_step: 100
visualize_per_epoch: 50
eval_per_epoch: 50
checkpoint_per_epoch: 1000
# load_from: /kmh-nfs-us-mount/logs/sqa/sqa_Flow_matching/20241209_173932_kkesfc_kmh-tpuvm-v2-32-6__b_lr_ep_eval/checkpoint_388000